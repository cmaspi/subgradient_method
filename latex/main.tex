\documentclass[twoside,twocolumn]{article}

\usepackage{blindtext} % Package to generate dummy text throughout this template 

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{tikz, pgfplots}

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Running title $\bullet$ May 2016 $\bullet$ Vol. XXI, No. 1} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\renewcommand{\vec}[1]{\underline{#1}}
\newcommand\twospace{\,\,}
\newcommand\fourspace{\,\,\,\,}
\newcommand\norm[1]{\ensuremath{\lVert#1\rVert_2}}
\newcommand\abs[1]{\ensuremath{\Vert#1\Vert}}
%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{Subgradient Method} % Article title
\author{%
\textsc{Chirag Mehta}\thanks{This is a preliminary report, the main report will be updated at \href{https://github.com/cmaspi/subgradient_method/blob/master/latex/main.pdf}{github}} \\[1ex] % Your name
\normalsize Indian Institute of Technology, Hyderabad \\ % Your institution
\normalsize \href{mailto:ai20btech11006@iith.ac.in}{ai20btech11006@iith.ac.in} % Your email address
%\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
%\textsc{Jane Smith}\thanks{Corresponding author} \\[1ex] % Second author's name
%\normalsize University of Utah \\ % Second author's institution
%\normalsize \href{mailto:jane@smith.com}{jane@smith.com} % Second author's email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
\begin{abstract}
\noindent
Non-differentiable functions are an important class of functions which often appear in optimization problems, a gradient descent method would fail to optimize such an objective because the gradient might not exist. Methods such as interior point method work great, but it has its own limitations of being computationally inefficient. We will explore the subgradient method which is an iterative first-order method similar to gradient descent. We will also be exploring the heavy-ball method to make the subgradient method faster. 
\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

\section{Introduction}
Subgradient method is a simple algorithm used to minimize non-differentiable convex functions. This method is similar to vanilla gradient method which is used to optimize differentiable functions. 
% can add few lines on comparison

\subsection{Algorithm}
Lets say we have a nondifferentiable function $f\, : \, \mathbb{R}^n\to \mathbb{R}$. The update rule of subgradient method says
\begin{align}
    \vec{x}^{(k+1)} = \vec{x}^{(k)} - \alpha_k\vec{g}^{(k)} \label{eq:subgrad_iter}
\end{align}
where $\vec{x}^{(k)}$ is the $k^{th}$ iterate, $\alpha_k$ is the step step size at $k^{th}$ iteration and $\vec{g}^{(k)}$ is any subgradient of $f$ at $\vec{x}^{(k)}$. The subgradient $\vec{g}^{(k)}$ is any vector which satisfies
\begin{align}
    f(\vec{y}) \geq f(\vec{x}) + \vec{g}^T(\vec{y}-\vec{x})\label{eq:subgrad_condition}
\end{align}
At a given point there can be more than one subgradients, we call the set of subgradients as subdifferential.
\begin{theorem}
    If the function f is differentiable at $\vec{x}^{(k)}$ then $\vec{g}^{(k)}$ is equal to the gradient of f at $\vec{x}^{(k)}$
\end{theorem}
\begin{proof}
    Substitute $\vec{y} = \vec{x}+\lambda\vec{z},\twospace \lambda > 0$ in \eqref{eq:subgrad_condition}
    \begin{align}
        \frac{f(\vec{x}+\lambda\vec{z}) - f(\vec{x})}{\lambda} \geq \vec{g}^T\vec{z}
    \end{align}
    We can use the limit $\lambda \to 0$
    \begin{align}
        \vec{\nabla}f(\vec{x})^T\vec{z} &\geq \vec{g}^T\vec{z}\\
        \vec{z}^T\left(\vec{\nabla}f(\vec{x})-g\right) &\geq 0 \fourspace \forall \vec{z}\\
        \therefore \vec{g} & = \vec{\nabla}f(\vec{x})
    \end{align}
\end{proof}

\section{SVM using subgradient method}
A support vector machine is used for two class classification. The objective is to maximize the slab thickness while still satisfying  few constraints. Hard Margin SVM problem can be formuated as follows
\begin{align}
    \begin{split}
        \min_{\vec{w},b}\, &\twospace\vec{w}^T\vec{w}
    \end{split}\\ 
    \text{s.t:}
    \twospace & y_i(\vec{w}^T\vec{x_i}+b)\geq 1\\
\end{align}
We can transform this problem into
\begin{align}
    \min_{\vec{w},b}\, &\twospace\vec{w}^T\vec{w} + \lambda \sum_i \max(0, 1-y_i)
\end{align}

\section{Convergence proof}
Lets assume $x^*$ is the minimizer of our objective function $f$. Assume that the norm of subgradients is bounded.

Using Lipschitz condition
\begin{align}
    \left| f(u)-f(v)\right| \leq G\norm{u-v}
\end{align}
for all $u,v$. Some versions of subgradient method work even when the gradient is not bounded. 
% expected structure
%-----------

% motivation for subgradient method
% a simple example where gradient descent wouldn't work

% correctness proof

% comparison with newton's method and interior point method

% A demonstration of sub gradient descent

% Applying subgradient descent in SVM

% Trying subgradient descent with other standard methods in gradient descent
% such as momentum ( heavy ball method)



%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------
%-----------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\section{Preliminary References}
\begin{itemize}
    \item \href{https://www.youtube.com/watch?v=jYtCiV1aP44}{A youtube video on subgradient method}
    \item \href{https://web.stanford.edu/class/ee392o/subgrad_method.pdf}{Stanford notes}
    \item \href{https://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/06-subgradients-scribed.pdf}{Mathematics behind subgradients}
    \item \href{https://mcneela.github.io/machine_learning/2020/04/24/Subgradient-Descent.html}{Subgradient method in SVM}
    \item \href{https://davidrosenberg.github.io/mlcourse/Archive/2018/Lectures/03c.subgradient-descent.pdf}{Subgradient method in SVM}
    \item \href{https://web.stanford.edu/class/ee364b/lectures/subgrad_method_notes.pdf}{Heavy-ball method}
\end{itemize}


\end{document}
